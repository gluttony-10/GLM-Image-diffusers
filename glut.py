import warnings
warnings.filterwarnings("ignore")
import os
from PIL import Image, PngImagePlugin
import time
import torch
import numpy as np
import random
import gradio as gr
import socket
import argparse
import datetime
import psutil
from glm_image import GlmImagePipeline
from diffusers.models import GlmImageTransformer2DModel
from diffusers.utils import load_image
from transformers import GlmImageForConditionalGeneration, ByT5Tokenizer

parser = argparse.ArgumentParser() 
parser.add_argument("--server_name", type=str, default="127.0.0.1", help="IPåœ°å€ï¼Œå±€åŸŸç½‘è®¿é—®æ”¹ä¸º0.0.0.0")
parser.add_argument("--server_port", type=int, default=7891, help="ä½¿ç”¨ç«¯å£")
parser.add_argument("--share", action="store_true", help="æ˜¯å¦å¯ç”¨gradioå…±äº«")
parser.add_argument("--compile", action="store_true", help="æ˜¯å¦å¯ç”¨compileåŠ é€Ÿ")
parser.add_argument("--res_vram", type=int, default=1000, help="ä¿ç•™æ˜¾å­˜(MB)ï¼Œé»˜è®¤1000")
args = parser.parse_args()

print(" å¯åŠ¨ä¸­ï¼Œè¯·è€å¿ƒç­‰å¾… bilibili@åå­—é±¼ https://space.bilibili.com/893892")
print(f'\033[32mPytorchç‰ˆæœ¬ï¼š{torch.__version__}\033[0m')
if torch.cuda.is_available():
    device = "cuda" 
    print(f'\033[32mæ˜¾å¡å‹å·ï¼š{torch.cuda.get_device_name()}\033[0m')
    total_vram_in_gb = torch.cuda.get_device_properties(0).total_memory / 1073741824
    print(f'\033[32mæ˜¾å­˜å¤§å°ï¼š{total_vram_in_gb:.2f}GB\033[0m')
    mem = psutil.virtual_memory()
    print(f'\033[32må†…å­˜å¤§å°ï¼š{mem.total/1073741824:.2f}GB\033[0m')
    if torch.cuda.get_device_capability()[0] >= 8:
        print(f'\033[32mæ”¯æŒBF16\033[0m')
        dtype = torch.bfloat16
    else:
        print(f'\033[32mä¸æ”¯æŒBF16ï¼Œä½¿ç”¨FP32\033[0m')
        dtype = torch.float32
else:
    print(f'\033[32mCUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥\033[0m')
    device = "cpu"
    dtype = torch.float32
    mem = psutil.virtual_memory()
    print(f'\033[32må†…å­˜å¤§å°ï¼š{mem.total/1073741824:.2f}GB\033[0m')

# åˆå§‹åŒ–
pipe = None
mmgp = None
stop_generation = False
model_id = "models/GLM-Image-diffusers"

# vision_language_encoder ç¼“å­˜ï¼ˆç¼“å­˜ prior_tokens å’Œ prompt_embedsï¼‰
prior_cache = {
    "key": None,  # (prompt, height, width, image_hash)
    "prompt": None,  # ç”¨äº prompt_embeds ç¼“å­˜
    "prior_token_ids": None,
    "prior_image_token_ids": None,
    "prompt_embeds": None,
}

# å¯ç”¨ CUDA åŠ é€Ÿä¼˜åŒ–
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True  # è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜å·ç§¯ç®—æ³•
    torch.backends.cuda.matmul.allow_tf32 = True  # å…è®¸ TF32 çŸ©é˜µä¹˜æ³•
    torch.backends.cudnn.allow_tf32 = True  # å…è®¸ TF32 åŠ é€Ÿ


def get_image_hash(img):
    """è·å–å›¾åƒçš„ç®€å•å“ˆå¸Œå€¼ç”¨äºç¼“å­˜"""
    if img is None:
        return None
    # ä½¿ç”¨å›¾åƒå°ºå¯¸å’Œéƒ¨åˆ†åƒç´ æ•°æ®ç”Ÿæˆç®€å•å“ˆå¸Œ
    return hash((img.size, img.mode, img.tobytes()[:1000]))


def get_cached_prompt_embeds(prompt):
    """è·å–ç¼“å­˜çš„ prompt_embeds"""
    global prior_cache
    
    # æ£€æŸ¥ç¼“å­˜ï¼ˆåªåŸºäº promptï¼‰
    if prior_cache["prompt"] == prompt and prior_cache["prompt_embeds"] is not None:
        print("ğŸ“¦ ä½¿ç”¨ç¼“å­˜çš„ prompt_embeds")
        return prior_cache["prompt_embeds"]
    
    # ç¼–ç æ–°çš„æç¤ºè¯
    print("ğŸ”„ ç¼–ç æç¤ºè¯...")
    with torch.inference_mode():
        prompt_embeds, _ = pipe.encode_prompt(
            prompt=prompt,
            do_classifier_free_guidance=False,
        )
    
    # æ›´æ–°ç¼“å­˜
    prior_cache["prompt"] = prompt
    prior_cache["prompt_embeds"] = prompt_embeds
    
    return prompt_embeds


def get_cached_prior_tokens(prompt, height, width, image=None):
    """è·å–ç¼“å­˜çš„ prior tokensï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”Ÿæˆ"""
    global prior_cache
    
    # ç”Ÿæˆç¼“å­˜é”®
    image_hash = get_image_hash(image)
    cache_key = (prompt, height, width, image_hash)
    
    # æ£€æŸ¥ç¼“å­˜
    if prior_cache["key"] == cache_key and prior_cache["prior_token_ids"] is not None:
        print("ğŸ“¦ ä½¿ç”¨ç¼“å­˜çš„ vision_language_encoder ç»“æœ")
        return (
            prior_cache["prior_token_ids"],
            prior_cache["prior_image_token_ids"],
            prior_cache["prompt_embeds"],
        )
    
    # ç”Ÿæˆæ–°çš„ prior tokens
    # print("ğŸ”„ ç¼–ç æç¤ºè¯å’Œç”Ÿæˆ prior tokensï¼ˆæ— è¿›åº¦æ¡ï¼Œæ—¶é—´è¾ƒé•¿ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰...")
    prior_token_ids = None
    prior_image_token_ids = None
    prompt_embeds = None
    
    try:
        with torch.inference_mode():
            # ç”Ÿæˆ prior tokensï¼ˆè¿™æ˜¯è€—æ—¶çš„ vision_language_encoder æ“ä½œï¼‰
            # æ€»æ˜¯è¿”å› (prior_token_ids, prior_image_token_ids) å…ƒç»„
            prior_token_ids, prior_image_token_ids = pipe.generate_prior_tokens(
                prompt=prompt,
                height=height,
                width=width,
                image=[image] if image is not None else None,
            )
            
            # ç¼–ç æç¤ºè¯
            prompt_embeds, _ = pipe.encode_prompt(
                prompt=prompt,
                do_classifier_free_guidance=False,
            )
    except Exception as e:
        print(f"âŒ ç”Ÿæˆ prior tokens å¤±è´¥: {e}")
        raise
    
    # æ›´æ–°ç¼“å­˜
    prior_cache["key"] = cache_key
    prior_cache["prior_token_ids"] = prior_token_ids
    prior_cache["prior_image_token_ids"] = prior_image_token_ids
    prior_cache["prompt_embeds"] = prompt_embeds
    
    return prior_token_ids, prior_image_token_ids, prompt_embeds



# ç¡®ä¿è¾“å‡ºæ–‡ä»¶å¤¹å­˜åœ¨
os.makedirs("outputs", exist_ok=True)

# é»˜è®¤è®¾ç½®
num_inference_steps_default = 50
guidance_scale_default = 1.5
width_default = 1024
height_default = 1024
res_vram = args.res_vram


def load_pipeline():
    """åŠ è½½ GLM-Image ç®¡é“"""
    global pipe, mmgp
    if pipe is None:
        print("æ­£åœ¨åŠ è½½ GLM-Image æ¨¡å‹...")
        try:
            # é‡åŒ–æ¨¡å‹è·¯å¾„
            vision_lang_encoder_path = "models/GLM-Image-diffusers/vision_language_encoder-mmgp.safetensors"
            transformer_path = "models/GLM-Image-diffusers/transformer-mmgp.safetensors"
            
            # åŠ è½½ tokenizer
            tokenizer = ByT5Tokenizer.from_pretrained(
                model_id,
                subfolder="tokenizer",
                use_fast=False,
            )
            
            # å…ˆåŠ è½½åŸºç¡€æ¨¡å‹åˆ° CPU
            pipe = GlmImagePipeline.from_pretrained(
                model_id, 
                vision_language_encoder = None,
                transformer = None,
                tokenizer = tokenizer,
                torch_dtype=dtype,
            ).to("cpu")
            
            # åŠ è½½å®Œæˆåå†å¯¼å…¥ mmgp
            from mmgp import offload
            # ä½¿ç”¨ mmgp å¿«é€ŸåŠ è½½é‡åŒ–æ¨¡å‹
            if hasattr(pipe, 'vision_language_encoder'):
                pipe.vision_language_encoder = offload.fast_load_transformers_model(
                    vision_lang_encoder_path,
                    modelClass=GlmImageForConditionalGeneration,
                    forcedConfigPath=f"{model_id}/vision_language_encoder/config.json",
                )
            pipe.transformer = offload.fast_load_transformers_model(
                transformer_path,
                modelClass=GlmImageTransformer2DModel,
                forcedConfigPath=f"{model_id}/transformer/config.json",
            )
            
            # è®¡ç®—æ˜¾å­˜é¢„ç®—
            if device == "cuda":
                free_memory, _ = torch.cuda.mem_get_info(0)
                budgets = int(free_memory / 1048576 - res_vram)  # è½¬æ¢ä¸º MB
            else:
                budgets = 0
            
            # é…ç½® mmgpï¼ˆä¸é‡æ–°é‡åŒ–ï¼‰
            mmgp = offload.all(
                pipe, 
                pinnedMemory=["vision_language_encoder", "text_encoder", "transformer"] if mem.total/1073741824 > 30 else ["transformer"],
                budgets={'*': budgets}, 
                extraModelsToQuantize=["vision_language_encoder"], 
                compile=True if args.compile else False,
            )
            
            # å¯ç”¨ Channels Last å†…å­˜æ ¼å¼åŠ é€Ÿ
            if device == "cuda" and hasattr(pipe, 'transformer'):
                try:
                    pipe.transformer = pipe.transformer.to(memory_format=torch.channels_last)
                    print("âœ… Channels Last å†…å­˜æ ¼å¼å·²å¯ç”¨")
                except Exception as e:
                    print(f"âš ï¸ Channels Last å¯ç”¨å¤±è´¥: {e}")
            
            print("âœ… é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆï¼mmgp é…ç½®å®Œæˆï¼Œé™åˆ¶ç›®æ ‡æ˜¾å­˜ï¼š" + str(budgets) + "MB")
                
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    return pipe


# è§£å†³å†²çªç«¯å£
def find_port(port: int) -> int:
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        if s.connect_ex(("localhost", port)) == 0:
            print(f"ç«¯å£ {port} å·²è¢«å ç”¨ï¼Œæ­£åœ¨å¯»æ‰¾å¯ç”¨ç«¯å£...")
            return find_port(port=port + 1)
        else:
            return port


def stop_generate():
    """åœæ­¢ç”Ÿæˆ"""
    global stop_generation
    stop_generation = True
    return "âœ… ç”Ÿæˆå·²åœæ­¢"


def generate_t2i(prompt, negative_prompt, width, height, num_inference_steps, 
                 guidance_scale, seed_param, batch_images):
    """æ–‡æœ¬ç”Ÿæˆå›¾åƒ"""
    global stop_generation, pipe
    
    if not prompt or not prompt.strip():
        yield None, "âŒ è¯·è¾“å…¥æç¤ºè¯"
        return
    
    if pipe is None:
        pipe = load_pipeline()
        if pipe is None:
            yield None, "âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥"
            return
    
    stop_generation = False
    results = []
    inference_times = []
    start_time = time.time()
    
    # å¤„ç†ç§å­
    if seed_param < 0:
        seed = random.randint(0, np.iinfo(np.int32).max)
    else:
        seed = seed_param
    
    # ç¡®ä¿å®½é«˜æ˜¯32çš„å€æ•°
    width = (width // 32) * 32
    height = (height // 32) * 32
    
    start_msg = f"ğŸš€ å¼€å§‹ç”Ÿæˆï¼Œå…±{batch_images}å¼ ï¼Œåˆ†è¾¨ç‡{width}x{height}ï¼Œæ­¥æ•°{num_inference_steps}..."
    print(start_msg)
    yield None, start_msg
    
    # æ£€æŸ¥ç¼“å­˜çŠ¶æ€ï¼Œå¦‚æœæœªå‘½ä¸­åˆ™æç¤º
    cache_key = (prompt, height, width, get_image_hash(None))
    if prior_cache["key"] != cache_key or prior_cache["prior_token_ids"] is None:
        msg = "ğŸ”„ ç¼–ç æç¤ºè¯å’Œç”Ÿæˆ prior tokensï¼ˆæ— è¿›åº¦æ¡ï¼Œæ—¶é—´è¾ƒé•¿ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰..."
        print(msg)
        yield None, msg

    # è·å–ç¼“å­˜çš„ prior tokensï¼ˆæ–‡ç”Ÿå›¾æ—¶ prior_image_token_ids ä¸º Noneï¼‰
    prior_token_ids, prior_image_token_ids, prompt_embeds = get_cached_prior_tokens(
        prompt=prompt, height=height, width=width, image=None
    )
    
    try:
        for i in range(batch_images):
            if stop_generation:
                stop_generation = False
                yield results if results else None, f"âœ… ç”Ÿæˆå·²ä¸­æ­¢ï¼Œæœ€åç§å­æ•°{seed+i-1}"
                break
            
            current_seed = seed + i
            generator = torch.Generator(device=device).manual_seed(current_seed)
            
            # è®°å½•å•å¼ å›¾æ¨ç†å¼€å§‹æ—¶é—´
            img_start_time = time.time()
            
            # T2I ä½¿ç”¨ç¼“å­˜çš„ prior_token_ids å’Œ prompt_embeds
            with torch.inference_mode():
                # ä½¿ç”¨ yield_progress=True è·å–è¿›åº¦
                generator_obj = pipe(
                    prompt_embeds=prompt_embeds,
                    prior_token_ids=prior_token_ids,
                    height=height,
                    width=width,
                    num_inference_steps=num_inference_steps,
                    guidance_scale=guidance_scale,
                    generator=generator,
                    yield_progress=True
                )
                
                output = None
                for res, step, total in generator_obj:
                    if res is None:
                        # è¿›åº¦æ›´æ–°
                        progress_msg = f"ğŸš€ ç”Ÿæˆä¸­ {step}/{total}..."
                        yield results if results else None, progress_msg
                    else:
                        # å®Œæˆ
                        output = res
            
            # è®°å½•å•å¼ å›¾æ¨ç†æ—¶é—´
            img_time = time.time() - img_start_time
            inference_times.append(img_time)
            
            image = output.images[0]
            
            # ä¿å­˜å›¾åƒ
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"outputs/t2i_{timestamp}_{current_seed}.png"
            
            # æ·»åŠ å…ƒæ•°æ®
            pnginfo = PngImagePlugin.PngInfo()
            pnginfo.add_text("mode", "t2i\n")
            pnginfo.add_text("prompt", f"{prompt}\n")
            pnginfo.add_text("negative_prompt", f"{negative_prompt}\n")
            pnginfo.add_text("width", f"{width}\n")
            pnginfo.add_text("height", f"{height}\n")
            pnginfo.add_text("num_inference_steps", f"{num_inference_steps}\n")
            pnginfo.add_text("guidance_scale", f"{guidance_scale}\n")
            pnginfo.add_text("seed", f"{current_seed}\n")
            
            image.save(filename, pnginfo=pnginfo)
            results.append(image)
            
            img_msg = f"âœ… ç¬¬{i+1}å¼ å®Œæˆï¼Œç§å­{current_seed}ï¼Œè€—æ—¶{img_time:.2f}ç§’"
            print(img_msg)
            yield results, img_msg
        
        # è®¡ç®—æ€»æ—¶é—´å’Œå¹³å‡æ—¶é—´
        total_time = time.time() - start_time
        avg_time = total_time / len(results) if results else 0
        done_msg = f"âœ… æ¨ç†å®Œæˆï¼Œå…±{len(results)}å¼ ï¼Œæ€»è€—æ—¶{total_time:.2f}ç§’ï¼Œå¹³å‡{avg_time:.2f}ç§’/å¼ "
        print(done_msg)
        yield results, done_msg
    
    except Exception as e:
        import traceback
        error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"
        print(error_msg)
        print("=" * 80)
        print("å®Œæ•´é”™è¯¯å †æ ˆ:")
        traceback.print_exc()
        print("=" * 80)
        yield results if results else None, error_msg


def generate_i2i(image, prompt, negative_prompt, width, height, num_inference_steps,
                 guidance_scale, seed_param, batch_images):
    """å›¾åƒåˆ°å›¾åƒç”Ÿæˆ"""
    global stop_generation, pipe
    
    if image is None:
        yield None, "âŒ è¯·ä¸Šä¼ è¾“å…¥å›¾åƒ"
        return
    
    if not prompt or not prompt.strip():
        yield None, "âŒ è¯·è¾“å…¥æç¤ºè¯"
        return
    
    if pipe is None:
        pipe = load_pipeline()
        if pipe is None:
            yield None, "âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥"
            return
    
    stop_generation = False
    results = []
    inference_times = []
    start_time = time.time()
    
    # å¤„ç†è¾“å…¥å›¾åƒ
    if isinstance(image, dict):
        image = image.get("background", image)
    image = load_image(image).convert("RGB")
    
    # å¤„ç†ç§å­
    if seed_param < 0:
        seed = random.randint(0, np.iinfo(np.int32).max)
    else:
        seed = seed_param
    
    # ç¡®ä¿å®½é«˜æ˜¯32çš„å€æ•°
    width = (width // 32) * 32
    height = (height // 32) * 32
    
    start_msg = f"ğŸš€ å¼€å§‹ç”Ÿæˆï¼Œå…±{batch_images}å¼ ï¼Œåˆ†è¾¨ç‡{width}x{height}ï¼Œæ­¥æ•°{num_inference_steps}..."
    print(start_msg)
    yield None, start_msg
    
    # æ£€æŸ¥ç¼“å­˜çŠ¶æ€
    cache_key = (prompt, height, width, get_image_hash(image))
    if prior_cache["key"] != cache_key or prior_cache["prior_token_ids"] is None:
        msg = "ğŸ”„ ç¼–ç æç¤ºè¯å’Œç”Ÿæˆ prior tokensï¼ˆæ— è¿›åº¦æ¡ï¼Œæ—¶é—´è¾ƒé•¿ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰..."
        print(msg)
        yield None, msg

    # è·å–ç¼“å­˜çš„ prior tokensï¼ˆåŒ…å« prior_token_idsã€prior_image_token_ids å’Œ prompt_embedsï¼‰
    prior_token_ids, prior_image_token_ids, prompt_embeds = get_cached_prior_tokens(
        prompt=prompt, height=height, width=width, image=image
    )
    
    try:
        for i in range(batch_images):
            if stop_generation:
                stop_generation = False
                yield results if results else None, f"âœ… ç”Ÿæˆå·²ä¸­æ­¢ï¼Œæœ€åç§å­æ•°{seed+i-1}"
                break
            
            current_seed = seed + i
            generator = torch.Generator(device=device).manual_seed(current_seed)
            
            # è®°å½•å•å¼ å›¾æ¨ç†å¼€å§‹æ—¶é—´
            img_start_time = time.time()
            
            # ä½¿ç”¨ç¼“å­˜çš„ prior_token_idsã€prior_image_token_ids å’Œ prompt_embeds
            with torch.inference_mode():
                # ä½¿ç”¨ yield_progress=True è·å–è¿›åº¦
                generator_obj = pipe(
                    prompt_embeds=prompt_embeds,
                    prior_token_ids=prior_token_ids,
                    prior_image_token_ids=prior_image_token_ids,
                    image=[image],
                    height=height,
                    width=width,
                    num_inference_steps=num_inference_steps,
                    guidance_scale=guidance_scale,
                    generator=generator,
                    yield_progress=True
                )
                
                output = None
                for res, step, total in generator_obj:
                    if res is None:
                        # è¿›åº¦æ›´æ–°
                        progress_msg = f"ğŸš€ ç”Ÿæˆä¸­ {step}/{total}..."
                        yield results if results else None, progress_msg
                    else:
                        # å®Œæˆ
                        output = res
            
            # è®°å½•å•å¼ å›¾æ¨ç†æ—¶é—´
            img_time = time.time() - img_start_time
            inference_times.append(img_time)
            
            generated_image = output.images[0]
            
            # ä¿å­˜å›¾åƒ
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"outputs/i2i_{timestamp}_{current_seed}.png"
            
            # æ·»åŠ å…ƒæ•°æ®
            pnginfo = PngImagePlugin.PngInfo()
            pnginfo.add_text("mode", "i2i\n")
            pnginfo.add_text("prompt", f"{prompt}\n")
            pnginfo.add_text("negative_prompt", f"{negative_prompt}\n")
            pnginfo.add_text("width", f"{width}\n")
            pnginfo.add_text("height", f"{height}\n")
            pnginfo.add_text("num_inference_steps", f"{num_inference_steps}\n")
            pnginfo.add_text("guidance_scale", f"{guidance_scale}\n")
            pnginfo.add_text("seed", f"{current_seed}\n")
            
            generated_image.save(filename, pnginfo=pnginfo)
            results.append(generated_image)
            
            img_msg = f"âœ… ç¬¬{i+1}å¼ å®Œæˆï¼Œç§å­{current_seed}ï¼Œè€—æ—¶{img_time:.2f}ç§’"
            print(img_msg)
            yield results, img_msg
        
        # è®¡ç®—æ€»æ—¶é—´å’Œå¹³å‡æ—¶é—´
        total_time = time.time() - start_time
        avg_time = total_time / len(results) if results else 0
        done_msg = f"âœ… æ¨ç†å®Œæˆï¼Œå…±{len(results)}å¼ ï¼Œæ€»è€—æ—¶{total_time:.2f}ç§’ï¼Œå¹³å‡{avg_time:.2f}ç§’/å¼ "
        print(done_msg)
        yield results, done_msg
    
    except Exception as e:
        import traceback
        error_msg = f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"
        print(error_msg)
        print("=" * 80)
        print("å®Œæ•´é”™è¯¯å †æ ˆ:")
        traceback.print_exc()
        print("=" * 80)
        yield results if results else None, error_msg


def exchange_width_height(width, height):
    """äº¤æ¢å®½é«˜"""
    return height, width, "âœ… å®½é«˜äº¤æ¢å®Œæ¯•"


def scale_resolution_1_5(width, height):
    """
    å°†å®½åº¦å’Œé«˜åº¦éƒ½æ”¾å¤§1.5å€ï¼Œå¹¶æŒ‰ç…§32çš„å€æ•°å‘ä¸‹å–æ•´
    """
    new_width = int(width * 1.5) // 32 * 32
    new_height = int(height * 1.5) // 32 * 32
    return new_width, new_height, "âœ… åˆ†è¾¨ç‡å·²è°ƒæ•´ä¸º1.5å€"


def calculate_dimensions(target_area, ratio):
    """
    æ ¹æ®ç›®æ ‡åƒç´ é¢ç§¯å’Œå®½é«˜æ¯”è®¡ç®—å®½é«˜
    """
    import math
    width = math.sqrt(target_area * ratio)
    height = width / ratio
    width = round(width / 32) * 32
    height = round(height / 32) * 32
    return int(width), int(height)


def auto_adjust_resolution(image):
    """
    æ ¹æ®ä¸Šä¼ çš„å›¾åƒè‡ªåŠ¨è°ƒæ•´å®½é«˜ï¼ˆç­‰æ•ˆ 1024x1024 åƒç´ é¢ç§¯ï¼‰
    """
    if image is None:
        return width_default, height_default, ""
    
    # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
    if isinstance(image, dict):
        image = image.get("background", image)
    
    # è·å–å›¾åƒå°ºå¯¸
    if hasattr(image, 'size'):  # PIL Image
        img_width, img_height = image.size
    elif hasattr(image, 'shape'):  # numpy array
        img_height, img_width = image.shape[:2]
    else:
        return width_default, height_default, ""
    
    # è®¡ç®—ç­‰æ•ˆ 1024x1024 çš„å°ºå¯¸ï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
    target_area = 1024 * 1024  # ç›®æ ‡åƒç´ é¢ç§¯
    ratio = img_width / img_height
    new_width, new_height = calculate_dimensions(target_area, ratio)
    
    # ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
    new_width = max(32, min(2048, new_width))
    new_height = max(32, min(2048, new_height))
    
    return new_width, new_height, f"âœ… å·²è°ƒæ•´ä¸º {new_width}x{new_height}ï¼ˆç­‰æ•ˆ1024Â²ï¼‰"


# åˆ›å»º Gradio ç•Œé¢
css = """
.gradio-container {
    font-family: 'IBM Plex Sans', sans-serif;
}
"""

with gr.Blocks() as demo:
    gr.Markdown("""
            <div>
                <h2 style="font-size: 30px;text-align: center;">GLM-Image</h2>
            </div>
            <div style="text-align: center;">
                åå­—é±¼
                <a href="https://space.bilibili.com/893892">ğŸŒbilibili</a> 
                |GLM-Image-diffusers
                <a href="https://github.com/gluttony-10/GLM-Image-diffusers">ğŸŒgithub</a> 
            </div>
            <div style="text-align: center; font-weight: bold; color: red;">
                âš ï¸ è¯¥æ¼”ç¤ºä»…ä¾›å­¦æœ¯ç ”ç©¶å’Œä½“éªŒä½¿ç”¨ã€‚
            </div>
            """)
    
    with gr.Tabs():
        # Text to Image æ ‡ç­¾é¡µ
        with gr.Tab("æ–‡ç”Ÿå›¾ "):
            with gr.Row():
                with gr.Column(scale=1):
                    prompt_t2i = gr.Textbox(
                        label="æç¤ºè¯",
                        placeholder="è¾“å…¥æè¿°å›¾åƒçš„æ–‡æœ¬æç¤ºè¯...",
                        lines=1,
                        value="A beautifully designed modern food magazine style dessert recipe illustration, themed around a raspberry mousse cake."
                    )
                    negative_prompt_t2i = gr.Textbox(
                        label="è´Ÿé¢æç¤ºè¯",
                        placeholder="è¾“å…¥ä¸å¸Œæœ›åœ¨å›¾åƒä¸­å‡ºç°çš„å†…å®¹...",
                        lines=1
                    )
                    
                    with gr.Row():
                        run_btn_t2i = gr.Button("ğŸ¨ ç”Ÿæˆå›¾åƒ", variant="primary", scale=2)
                        stop_button_t2i = gr.Button("â¹ï¸ åœæ­¢", scale=1)
                    
                    with gr.Row():
                        width_t2i = gr.Slider(
                            label="å®½åº¦",
                            minimum=32,
                            maximum=2048,
                            value=width_default,
                            step=32
                        )
                        height_t2i = gr.Slider(
                            label="é«˜åº¦",
                            minimum=32,
                            maximum=2048,
                            value=height_default,
                            step=32
                        )
                    
                    with gr.Row():
                        exchange_button_t2i = gr.Button("ğŸ”„ äº¤æ¢å®½é«˜", scale=1)
                    
                    num_inference_steps_t2i = gr.Slider(
                        label="æ¨ç†æ­¥æ•°",
                        minimum=1,
                        maximum=100,
                        value=num_inference_steps_default,
                        step=1
                    )
                    guidance_scale_t2i = gr.Slider(
                        label="å¼•å¯¼å¼ºåº¦",
                        minimum=0.1,
                        maximum=10.0,
                        value=guidance_scale_default,
                        step=0.1
                    )
                    
                    with gr.Row():
                        seed_t2i = gr.Number(
                            label="ç§å­ (-1ä¸ºéšæœº)",
                            value=-1,
                            precision=0
                        )
                        batch_images_t2i = gr.Slider(
                            label="ç”Ÿæˆæ•°é‡",
                            minimum=1,
                            maximum=10,
                            value=1,
                            step=1
                        )
                    



                
                with gr.Column(scale=1):
                    result_t2i = gr.Gallery(
                        label="ç”Ÿæˆç»“æœ",
                        show_label=True,
                        elem_id="gallery_t2i",
                        columns=2,
                        rows=2,
                        height="auto"
                    )
                    info_t2i = gr.Textbox(
                        label="ä¿¡æ¯",
                        lines=3,
                        interactive=False
                    )
        
        # Image to Image æ ‡ç­¾é¡µ
        with gr.Tab("å›¾ç”Ÿå›¾"):
            with gr.Row():
                with gr.Column(scale=1):
                    image_i2i = gr.Image(
                        label="è¾“å…¥å›¾åƒ",
                        type="pil",
                        sources=["upload", "clipboard"],
                        height=500
                    )
                    
                    prompt_i2i = gr.Textbox(
                        label="æç¤ºè¯",
                        placeholder="è¾“å…¥æè¿°æƒ³è¦ä¿®æ”¹çš„å†…å®¹...",
                        lines=1,
                        value="Replace the background of the snow forest with an underground station featuring an automatic escalator."
                    )
                    negative_prompt_i2i = gr.Textbox(
                        label="è´Ÿé¢æç¤ºè¯",
                        placeholder="è¾“å…¥ä¸å¸Œæœ›åœ¨å›¾åƒä¸­å‡ºç°çš„å†…å®¹...",
                        lines=1
                    )
                    
                    with gr.Row():
                        run_btn_i2i = gr.Button("ğŸ¨ ç”Ÿæˆå›¾åƒ", variant="primary", scale=2)
                        stop_button_i2i = gr.Button("â¹ï¸ åœæ­¢", scale=1)
                    
                    with gr.Row():
                        width_i2i = gr.Slider(
                            label="å®½åº¦",
                            minimum=32,
                            maximum=2048,
                            value=width_default,
                            step=32
                        )
                        height_i2i = gr.Slider(
                            label="é«˜åº¦",
                            minimum=32,
                            maximum=2048,
                            value=height_default,
                            step=32
                        )
                    
                    with gr.Row():
                        exchange_button_i2i = gr.Button("ğŸ”„ äº¤æ¢å®½é«˜", scale=1)
                        scale_1_5_button_i2i = gr.Button("ğŸ“Š 1.5åˆ†è¾¨ç‡", scale=1)
                    
                    num_inference_steps_i2i = gr.Slider(
                        label="æ¨ç†æ­¥æ•°",
                        minimum=1,
                        maximum=100,
                        value=num_inference_steps_default,
                        step=1
                    )
                    guidance_scale_i2i = gr.Slider(
                        label="å¼•å¯¼å¼ºåº¦",
                        minimum=0.1,
                        maximum=10.0,
                        value=guidance_scale_default,
                        step=0.1
                    )
                    
                    with gr.Row():
                        seed_i2i = gr.Number(
                            label="ç§å­ (-1ä¸ºéšæœº)",
                            value=-1,
                            precision=0
                        )
                        batch_images_i2i = gr.Slider(
                            label="ç”Ÿæˆæ•°é‡",
                            minimum=1,
                            maximum=10,
                            value=1,
                            step=1
                        )
                    



                
                with gr.Column(scale=1):
                    info_i2i = gr.Textbox(
                        label="ä¿¡æ¯",
                        lines=3,
                        interactive=False
                    )
                    result_i2i = gr.Gallery(
                        label="ç”Ÿæˆç»“æœ",
                        show_label=True,
                        elem_id="gallery_i2i",
                        columns=2,
                        rows=2,
                        height="auto"
                    )
                    
    
    # ç»‘å®šäº‹ä»¶
    # T2I äº‹ä»¶
    gr.on(
        triggers=[run_btn_t2i.click, prompt_t2i.submit, negative_prompt_t2i.submit],
        fn=generate_t2i,
        inputs=[prompt_t2i, negative_prompt_t2i, width_t2i, height_t2i, 
                num_inference_steps_t2i, guidance_scale_t2i, seed_t2i, batch_images_t2i],
        outputs=[result_t2i, info_t2i]
    )
    
    stop_button_t2i.click(
        fn=stop_generate,
        inputs=[],
        outputs=[info_t2i]
    )
    
    exchange_button_t2i.click(
        fn=exchange_width_height,
        inputs=[width_t2i, height_t2i],
        outputs=[width_t2i, height_t2i, info_t2i]
    )
    
    # I2I äº‹ä»¶
    gr.on(
        triggers=[run_btn_i2i.click, prompt_i2i.submit, negative_prompt_i2i.submit],
        fn=generate_i2i,
        inputs=[image_i2i, prompt_i2i, negative_prompt_i2i, width_i2i, height_i2i,
                num_inference_steps_i2i, guidance_scale_i2i, seed_i2i, batch_images_i2i],
        outputs=[result_i2i, info_i2i]
    )
    
    stop_button_i2i.click(
        fn=stop_generate,
        inputs=[],
        outputs=[info_i2i]
    )
    
    exchange_button_i2i.click(
        fn=exchange_width_height,
        inputs=[width_i2i, height_i2i],
        outputs=[width_i2i, height_i2i, info_i2i]
    )
    
    scale_1_5_button_i2i.click(
        fn=scale_resolution_1_5,
        inputs=[width_i2i, height_i2i],
        outputs=[width_i2i, height_i2i, info_i2i]
    )
    
    # ä¸Šä¼ å›¾åƒåè‡ªåŠ¨è°ƒæ•´å®½é«˜
    image_i2i.change(
        fn=auto_adjust_resolution,
        inputs=[image_i2i],
        outputs=[width_i2i, height_i2i, info_i2i]
    )


if __name__ == "__main__":
    # é¢„åŠ è½½æ¨¡å‹
    print("æ­£åœ¨é¢„åŠ è½½æ¨¡å‹...")
    load_pipeline()
    
    demo.launch(
        server_name=args.server_name,
        server_port=find_port(args.server_port),
        share=args.share,
        inbrowser=True,
        css=css, 
        theme=gr.themes.Soft(font=[gr.themes.GoogleFont("IBM Plex Sans")]),
    )
