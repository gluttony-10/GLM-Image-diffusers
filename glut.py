import warnings
warnings.filterwarnings("ignore")
import gc
import os
from PIL import Image, PngImagePlugin
import json
import time
import torch
import numpy as np
import random
import gradio as gr
import socket
import argparse
import datetime
import psutil
from diffusers.pipelines.glm_image import GlmImagePipeline
from diffusers.models import GlmImageTransformer2DModel
from diffusers.utils import load_image
from transformers import GlmImageForConditionalGeneration

parser = argparse.ArgumentParser() 
parser.add_argument("--server_name", type=str, default="127.0.0.1", help="IPåœ°å€ï¼Œå±€åŸŸç½‘è®¿é—®æ”¹ä¸º0.0.0.0")
parser.add_argument("--server_port", type=int, default=7891, help="ä½¿ç”¨ç«¯å£")
parser.add_argument("--share", action="store_true", help="æ˜¯å¦å¯ç”¨gradioå…±äº«")
parser.add_argument("--compile", action="store_true", help="æ˜¯å¦å¯ç”¨compileåŠ é€Ÿ")
args = parser.parse_args()

print(" å¯åŠ¨ä¸­ï¼Œè¯·è€å¿ƒç­‰å¾… bilibili@åå­—é±¼ https://space.bilibili.com/893892")
print(f'\033[32mPytorchç‰ˆæœ¬ï¼š{torch.__version__}\033[0m')
if torch.cuda.is_available():
    device = "cuda" 
    print(f'\033[32mæ˜¾å¡å‹å·ï¼š{torch.cuda.get_device_name()}\033[0m')
    total_vram_in_gb = torch.cuda.get_device_properties(0).total_memory / 1073741824
    print(f'\033[32mæ˜¾å­˜å¤§å°ï¼š{total_vram_in_gb:.2f}GB\033[0m')
    mem = psutil.virtual_memory()
    print(f'\033[32må†…å­˜å¤§å°ï¼š{mem.total/1073741824:.2f}GB\033[0m')
    if torch.cuda.get_device_capability()[0] >= 8:
        print(f'\033[32mæ”¯æŒBF16\033[0m')
        dtype = torch.bfloat16
    else:
        print(f'\033[32mä¸æ”¯æŒBF16ï¼Œä½¿ç”¨FP32\033[0m')
        dtype = torch.float32
else:
    print(f'\033[32mCUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥\033[0m')
    device = "cpu"
    dtype = torch.float32
    mem = psutil.virtual_memory()
    print(f'\033[32må†…å­˜å¤§å°ï¼š{mem.total/1073741824:.2f}GB\033[0m')

# åˆå§‹åŒ–
pipe = None
mmgp = None
stop_generation = False
model_id = "models/GLM-Image-diffusers"

# ç¡®ä¿è¾“å‡ºæ–‡ä»¶å¤¹å­˜åœ¨
os.makedirs("outputs", exist_ok=True)

# è¯»å–è®¾ç½®
CONFIG_FILE = "config.json"
config = {}
if os.path.exists(CONFIG_FILE):
    with open(CONFIG_FILE, "r", encoding="utf-8") as f:
        config = json.load(f)

# é»˜è®¤è®¾ç½®
num_inference_steps_default = int(config.get("NUM_INFERENCE_STEPS", "50"))
guidance_scale_default = float(config.get("GUIDANCE_SCALE", "1.5"))
width_default = int(config.get("WIDTH", "1024"))
height_default = int(config.get("HEIGHT", "1024"))
res_vram = float(config.get("RES_VRAM", "1000"))


def load_pipeline():
    """åŠ è½½ GLM-Image ç®¡é“"""
    global pipe, mmgp
    if pipe is None:
        print("æ­£åœ¨åŠ è½½ GLM-Image æ¨¡å‹...")
        try:
            # é‡åŒ–æ¨¡å‹è·¯å¾„
            vision_lang_encoder_path = "models/GLM-Image-diffusers/vision_language_encoder-mmgp.safetensors"
            transformer_path = "models/GLM-Image-diffusers/transformer-mmgp.safetensors"
            
            # å…ˆåŠ è½½åŸºç¡€æ¨¡å‹åˆ° CPU
            pipe = GlmImagePipeline.from_pretrained(
                model_id, 
                vision_language_encoder = None,
                transformer = None,
                torch_dtype=dtype,
            ).to("cpu")
            
            # åŠ è½½å®Œæˆåå†å¯¼å…¥ mmgp
            from mmgp import offload
            # ä½¿ç”¨ mmgp å¿«é€ŸåŠ è½½é‡åŒ–æ¨¡å‹
            if hasattr(pipe, 'vision_language_encoder'):
                pipe.vision_language_encoder = offload.fast_load_transformers_model(
                    vision_lang_encoder_path,
                    modelClass=GlmImageForConditionalGeneration,
                    forcedConfigPath=f"{model_id}/vision_language_encoder/config.json",
                )
            pipe.transformer = offload.fast_load_transformers_model(
                transformer_path,
                modelClass=GlmImageTransformer2DModel,
                forcedConfigPath=f"{model_id}/transformer/config.json",
            )
            
            # è®¡ç®—æ˜¾å­˜é¢„ç®—
            if device == "cuda":
                free_memory, _ = torch.cuda.mem_get_info(0)
                budgets = int(free_memory / 1048576 - res_vram)  # è½¬æ¢ä¸º MB
            else:
                budgets = 0
            
            # é…ç½® mmgpï¼ˆä¸é‡æ–°é‡åŒ–ï¼‰
            mmgp = offload.all(
                pipe, 
                pinnedMemory=["vision_language_encoder", "text_encoder", "transformer"] if mem.total/1073741824 > 30 else ["transformer"],
                budgets={'*': budgets}, 
                extraModelsToQuantize=["vision_language_encoder"], 
                compile=True if args.compile else False,
            )
            
            print("âœ… é‡åŒ–æ¨¡å‹åŠ è½½å®Œæˆï¼mmgp é…ç½®å®Œæˆï¼Œé™åˆ¶ç›®æ ‡æ˜¾å­˜ï¼š" + str(budgets) + "MB")
                
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    return pipe


# è§£å†³å†²çªç«¯å£
def find_port(port: int) -> int:
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        if s.connect_ex(("localhost", port)) == 0:
            print(f"ç«¯å£ {port} å·²è¢«å ç”¨ï¼Œæ­£åœ¨å¯»æ‰¾å¯ç”¨ç«¯å£...")
            return find_port(port=port + 1)
        else:
            return port


def stop_generate():
    """åœæ­¢ç”Ÿæˆ"""
    global stop_generation
    stop_generation = True
    return "âœ… ç”Ÿæˆå·²åœæ­¢"


def generate_t2i(prompt, negative_prompt, width, height, num_inference_steps, 
                 guidance_scale, seed_param, batch_images):
    """æ–‡æœ¬ç”Ÿæˆå›¾åƒ"""
    global stop_generation, pipe
    
    if not prompt or not prompt.strip():
        yield None, "âŒ è¯·è¾“å…¥æç¤ºè¯"
        return
    
    if pipe is None:
        pipe = load_pipeline()
        if pipe is None:
            yield None, "âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥"
            return
    
    stop_generation = False
    results = []
    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
    
    # å¤„ç†ç§å­
    if seed_param < 0:
        seed = random.randint(0, np.iinfo(np.int32).max)
    else:
        seed = seed_param
    
    # ç¡®ä¿å®½é«˜æ˜¯32çš„å€æ•°
    width = (width // 32) * 32
    height = (height // 32) * 32
    
    try:
        for i in range(batch_images):
            if stop_generation:
                stop_generation = False
                yield results if results else None, f"âœ… ç”Ÿæˆå·²ä¸­æ­¢ï¼Œæœ€åç§å­æ•°{seed+i-1}"
                break
            
            current_seed = seed + i
            generator = torch.Generator(device=device).manual_seed(current_seed)
            
            # ç”Ÿæˆå›¾åƒ
            output = pipe(
                prompt=prompt,
                height=height,
                width=width,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                generator=generator,
            )
            
            image = output.images[0]
            
            # ä¿å­˜å›¾åƒ
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"outputs/t2i_{timestamp}_{current_seed}.png"
            
            # æ·»åŠ å…ƒæ•°æ®
            pnginfo = PngImagePlugin.PngInfo()
            pnginfo.add_text("mode", "t2i\n")
            pnginfo.add_text("prompt", f"{prompt}\n")
            pnginfo.add_text("negative_prompt", f"{negative_prompt}\n")
            pnginfo.add_text("width", f"{width}\n")
            pnginfo.add_text("height", f"{height}\n")
            pnginfo.add_text("num_inference_steps", f"{num_inference_steps}\n")
            pnginfo.add_text("guidance_scale", f"{guidance_scale}\n")
            pnginfo.add_text("seed", f"{current_seed}\n")
            
            image.save(filename, pnginfo=pnginfo)
            results.append(image)
            
            yield results, f"âœ… ç§å­æ•°{current_seed}ï¼Œä¿å­˜åœ°å€: {filename}"
            
            # mmgp ä¼šè‡ªåŠ¨ç®¡ç†æ˜¾å­˜ï¼Œè¿™é‡Œåªéœ€è¦æ¸…ç† Python å¯¹è±¡
            gc.collect()
        
        # è®¡ç®—æ€»æ—¶é—´
        end_time = time.time()
        total_time = end_time - start_time
        yield results, f"âœ… æ¨ç†å®Œæˆï¼Œå…±ç”Ÿæˆ{len(results)}å¼ å›¾ç‰‡ï¼Œæ€»è€—æ—¶{total_time:.2f}ç§’"
    
    except Exception as e:
        yield results if results else None, f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


def generate_i2i(image, prompt, negative_prompt, width, height, num_inference_steps,
                 guidance_scale, seed_param, batch_images):
    """å›¾åƒåˆ°å›¾åƒç”Ÿæˆ"""
    global stop_generation, pipe
    
    if image is None:
        yield None, "âŒ è¯·ä¸Šä¼ è¾“å…¥å›¾åƒ"
        return
    
    if not prompt or not prompt.strip():
        yield None, "âŒ è¯·è¾“å…¥æç¤ºè¯"
        return
    
    if pipe is None:
        pipe = load_pipeline()
        if pipe is None:
            yield None, "âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥"
            return
    
    stop_generation = False
    results = []
    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
    
    # å¤„ç†è¾“å…¥å›¾åƒ
    if isinstance(image, dict):
        image = image.get("background", image)
    image = load_image(image).convert("RGB")
    
    # å¤„ç†ç§å­
    if seed_param < 0:
        seed = random.randint(0, np.iinfo(np.int32).max)
    else:
        seed = seed_param
    
    # ç¡®ä¿å®½é«˜æ˜¯32çš„å€æ•°
    width = (width // 32) * 32
    height = (height // 32) * 32
    
    try:
        for i in range(batch_images):
            if stop_generation:
                stop_generation = False
                yield results if results else None, f"âœ… ç”Ÿæˆå·²ä¸­æ­¢ï¼Œæœ€åç§å­æ•°{seed+i-1}"
                break
            
            current_seed = seed + i
            generator = torch.Generator(device=device).manual_seed(current_seed)
            
            # ç”Ÿæˆå›¾åƒ
            output = pipe(
                prompt=prompt,
                image=[image],  # å¯ä»¥è¾“å…¥å¤šä¸ªå›¾åƒï¼Œå¦‚ [image, image1]
                height=height,
                width=width,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                generator=generator,
            )
            
            generated_image = output.images[0]
            
            # ä¿å­˜å›¾åƒ
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"outputs/i2i_{timestamp}_{current_seed}.png"
            
            # æ·»åŠ å…ƒæ•°æ®
            pnginfo = PngImagePlugin.PngInfo()
            pnginfo.add_text("mode", "i2i\n")
            pnginfo.add_text("prompt", f"{prompt}\n")
            pnginfo.add_text("negative_prompt", f"{negative_prompt}\n")
            pnginfo.add_text("width", f"{width}\n")
            pnginfo.add_text("height", f"{height}\n")
            pnginfo.add_text("num_inference_steps", f"{num_inference_steps}\n")
            pnginfo.add_text("guidance_scale", f"{guidance_scale}\n")
            pnginfo.add_text("seed", f"{current_seed}\n")
            
            generated_image.save(filename, pnginfo=pnginfo)
            results.append(generated_image)
            
            yield results, f"âœ… ç§å­æ•°{current_seed}ï¼Œä¿å­˜åœ°å€: {filename}"
            
            # mmgp ä¼šè‡ªåŠ¨ç®¡ç†æ˜¾å­˜ï¼Œè¿™é‡Œåªéœ€è¦æ¸…ç† Python å¯¹è±¡
            gc.collect()
        
        # è®¡ç®—æ€»æ—¶é—´
        end_time = time.time()
        total_time = end_time - start_time
        yield results, f"âœ… æ¨ç†å®Œæˆï¼Œå…±ç”Ÿæˆ{len(results)}å¼ å›¾ç‰‡ï¼Œæ€»è€—æ—¶{total_time:.2f}ç§’"
    
    except Exception as e:
        yield results if results else None, f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"


def exchange_width_height(width, height):
    """äº¤æ¢å®½é«˜"""
    return height, width, "âœ… å®½é«˜äº¤æ¢å®Œæ¯•"


def scale_resolution_1_5(width, height):
    """
    å°†å®½åº¦å’Œé«˜åº¦éƒ½æ”¾å¤§1.5å€ï¼Œå¹¶æŒ‰ç…§32çš„å€æ•°å‘ä¸‹å–æ•´
    """
    new_width = int(width * 1.5) // 32 * 32
    new_height = int(height * 1.5) // 32 * 32
    return new_width, new_height, "âœ… åˆ†è¾¨ç‡å·²è°ƒæ•´ä¸º1.5å€"


# åˆ›å»º Gradio ç•Œé¢
css = """
.gradio-container {
    font-family: 'IBM Plex Sans', sans-serif;
}
"""

with gr.Blocks() as demo:
    gr.Markdown("""
            <div>
                <h2 style="font-size: 30px;text-align: center;">GLM-Image</h2>
            </div>
            <div style="text-align: center;">
                åå­—é±¼
                <a href="https://space.bilibili.com/893892">ğŸŒbilibili</a> 
                |GLM-Image-diffusers
                <a href="https://github.com/gluttony-10/GLM-Image-diffusers">ğŸŒgithub</a> 
            </div>
            <div style="text-align: center; font-weight: bold; color: red;">
                âš ï¸ è¯¥æ¼”ç¤ºä»…ä¾›å­¦æœ¯ç ”ç©¶å’Œä½“éªŒä½¿ç”¨ã€‚
            </div>
            """)
    
    with gr.Tabs():
        # Text to Image æ ‡ç­¾é¡µ
        with gr.Tab("æ–‡ç”Ÿå›¾ "):
            with gr.Row():
                with gr.Column(scale=1):
                    prompt_t2i = gr.Textbox(
                        label="æç¤ºè¯",
                        placeholder="è¾“å…¥æè¿°å›¾åƒçš„æ–‡æœ¬æç¤ºè¯...",
                        lines=1,
                        value="A beautifully designed modern food magazine style dessert recipe illustration, themed around a raspberry mousse cake."
                    )
                    negative_prompt_t2i = gr.Textbox(
                        label="è´Ÿé¢æç¤ºè¯",
                        placeholder="è¾“å…¥ä¸å¸Œæœ›åœ¨å›¾åƒä¸­å‡ºç°çš„å†…å®¹...",
                        lines=1
                    )
                    
                    with gr.Row():
                        width_t2i = gr.Slider(
                            label="å®½åº¦",
                            minimum=32,
                            maximum=2048,
                            value=width_default,
                            step=32
                        )
                        height_t2i = gr.Slider(
                            label="é«˜åº¦",
                            minimum=32,
                            maximum=2048,
                            value=height_default,
                            step=32
                        )
                    
                    with gr.Row():
                        exchange_button_t2i = gr.Button("ğŸ”„ äº¤æ¢å®½é«˜", scale=1)
                    
                    num_inference_steps_t2i = gr.Slider(
                        label="æ¨ç†æ­¥æ•°",
                        minimum=1,
                        maximum=100,
                        value=num_inference_steps_default,
                        step=1
                    )
                    guidance_scale_t2i = gr.Slider(
                        label="å¼•å¯¼å¼ºåº¦",
                        minimum=0.1,
                        maximum=10.0,
                        value=guidance_scale_default,
                        step=0.1
                    )
                    
                    with gr.Row():
                        seed_t2i = gr.Number(
                            label="ç§å­ (-1ä¸ºéšæœº)",
                            value=-1,
                            precision=0
                        )
                        batch_images_t2i = gr.Slider(
                            label="ç”Ÿæˆæ•°é‡",
                            minimum=1,
                            maximum=10,
                            value=1,
                            step=1
                        )
                    
                    with gr.Row():
                        run_btn_t2i = gr.Button("ğŸ¨ ç”Ÿæˆå›¾åƒ", variant="primary", scale=2)
                        stop_button_t2i = gr.Button("â¹ï¸ åœæ­¢", scale=1)
                
                with gr.Column(scale=1):
                    result_t2i = gr.Gallery(
                        label="ç”Ÿæˆç»“æœ",
                        show_label=True,
                        elem_id="gallery_t2i",
                        columns=2,
                        rows=2,
                        height="auto"
                    )
                    info_t2i = gr.Textbox(
                        label="ä¿¡æ¯",
                        lines=3,
                        interactive=False
                    )
        
        # Image to Image æ ‡ç­¾é¡µ
        with gr.Tab("å›¾ç”Ÿå›¾"):
            with gr.Row():
                with gr.Column(scale=1):
                    image_i2i = gr.Image(
                        label="è¾“å…¥å›¾åƒ",
                        type="pil",
                        sources=["upload", "clipboard"]
                    )
                    
                    prompt_i2i = gr.Textbox(
                        label="æç¤ºè¯",
                        placeholder="è¾“å…¥æè¿°æƒ³è¦ä¿®æ”¹çš„å†…å®¹...",
                        lines=1,
                        value="Replace the background of the snow forest with an underground station featuring an automatic escalator."
                    )
                    negative_prompt_i2i = gr.Textbox(
                        label="è´Ÿé¢æç¤ºè¯",
                        placeholder="è¾“å…¥ä¸å¸Œæœ›åœ¨å›¾åƒä¸­å‡ºç°çš„å†…å®¹...",
                        lines=1
                    )
                    
                    with gr.Row():
                        width_i2i = gr.Slider(
                            label="å®½åº¦",
                            minimum=32,
                            maximum=2048,
                            value=width_default,
                            step=32
                        )
                        height_i2i = gr.Slider(
                            label="é«˜åº¦",
                            minimum=32,
                            maximum=2048,
                            value=height_default,
                            step=32
                        )
                    
                    with gr.Row():
                        exchange_button_i2i = gr.Button("ğŸ”„ äº¤æ¢å®½é«˜", scale=1)
                        scale_1_5_button_i2i = gr.Button("ğŸ“Š 1.5åˆ†è¾¨ç‡", scale=1)
                    
                    num_inference_steps_i2i = gr.Slider(
                        label="æ¨ç†æ­¥æ•°",
                        minimum=1,
                        maximum=100,
                        value=num_inference_steps_default,
                        step=1
                    )
                    guidance_scale_i2i = gr.Slider(
                        label="å¼•å¯¼å¼ºåº¦",
                        minimum=0.1,
                        maximum=10.0,
                        value=guidance_scale_default,
                        step=0.1
                    )
                    
                    with gr.Row():
                        seed_i2i = gr.Number(
                            label="ç§å­ (-1ä¸ºéšæœº)",
                            value=-1,
                            precision=0
                        )
                        batch_images_i2i = gr.Slider(
                            label="ç”Ÿæˆæ•°é‡",
                            minimum=1,
                            maximum=10,
                            value=1,
                            step=1
                        )
                    
                    with gr.Row():
                        run_btn_i2i = gr.Button("ğŸ¨ ç”Ÿæˆå›¾åƒ", variant="primary", scale=2)
                        stop_button_i2i = gr.Button("â¹ï¸ åœæ­¢", scale=1)
                
                with gr.Column(scale=1):
                    result_i2i = gr.Gallery(
                        label="ç”Ÿæˆç»“æœ",
                        show_label=True,
                        elem_id="gallery_i2i",
                        columns=2,
                        rows=2,
                        height="auto"
                    )
                    info_i2i = gr.Textbox(
                        label="ä¿¡æ¯",
                        lines=3,
                        interactive=False
                    )
    
    # ç»‘å®šäº‹ä»¶
    # T2I äº‹ä»¶
    gr.on(
        triggers=[run_btn_t2i.click, prompt_t2i.submit, negative_prompt_t2i.submit],
        fn=generate_t2i,
        inputs=[prompt_t2i, negative_prompt_t2i, width_t2i, height_t2i, 
                num_inference_steps_t2i, guidance_scale_t2i, seed_t2i, batch_images_t2i],
        outputs=[result_t2i, info_t2i]
    )
    
    stop_button_t2i.click(
        fn=stop_generate,
        inputs=[],
        outputs=[info_t2i]
    )
    
    exchange_button_t2i.click(
        fn=exchange_width_height,
        inputs=[width_t2i, height_t2i],
        outputs=[width_t2i, height_t2i, info_t2i]
    )
    
    # I2I äº‹ä»¶
    gr.on(
        triggers=[run_btn_i2i.click, prompt_i2i.submit, negative_prompt_i2i.submit],
        fn=generate_i2i,
        inputs=[image_i2i, prompt_i2i, negative_prompt_i2i, width_i2i, height_i2i,
                num_inference_steps_i2i, guidance_scale_i2i, seed_i2i, batch_images_i2i],
        outputs=[result_i2i, info_i2i]
    )
    
    stop_button_i2i.click(
        fn=stop_generate,
        inputs=[],
        outputs=[info_i2i]
    )
    
    exchange_button_i2i.click(
        fn=exchange_width_height,
        inputs=[width_i2i, height_i2i],
        outputs=[width_i2i, height_i2i, info_i2i]
    )
    
    scale_1_5_button_i2i.click(
        fn=scale_resolution_1_5,
        inputs=[width_i2i, height_i2i],
        outputs=[width_i2i, height_i2i, info_i2i]
    )
    
    adjust_button_i2i.click(
        fn=adjust_width_height,
        inputs=[image_i2i],
        outputs=[width_i2i, height_i2i, info_i2i]
    )
    
    # ä¸Šä¼ å›¾ç‰‡æ—¶è‡ªåŠ¨è°ƒæ•´å®½é«˜
    image_i2i.upload(
        fn=adjust_width_height, 
        inputs=[image_i2i], 
        outputs=[width_i2i, height_i2i, info_i2i]
    )


if __name__ == "__main__":
    # é¢„åŠ è½½æ¨¡å‹
    print("æ­£åœ¨é¢„åŠ è½½æ¨¡å‹...")
    load_pipeline()
    
    demo.launch(
        server_name=args.server_name,
        server_port=find_port(args.server_port),
        share=args.share,
        inbrowser=True,
        css=css, 
        theme=gr.themes.Soft(font=[gr.themes.GoogleFont("IBM Plex Sans")]),
    )
